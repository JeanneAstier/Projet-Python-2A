{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Python : reconnaissance de mélanomes \n",
    "\n",
    "Louise Blart & Jeanne Astier\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Les grains de beauté, ou *nævus mélanocytaire*, sont des tâches présentes sur la peau. La plupart des grains de beauté sont bénins ; mais certains peuvent évoluter en tumeurs malignes, dans 10 cas pour 100 000 environ. Ils s'agit alors de __mélanomes__, une forme de cancer de la peau.\n",
    "\n",
    "Comme pour toutes les formes de cancer, un diagnostic précoce augmente l'efficacité du traitement : une mélanome détecté tôt peut être traité facilement, avant que la maladie n'évolue vers une forme mortelle. \n",
    "\n",
    "### Problématique\n",
    "\n",
    "Comment la data science peut-elle aider à la reconnaissance des mélanomes ? \n",
    "\n",
    "Ce projet a vocation à __aider au diagnostic de mélanomes__ : nous voulons détecter, parmi un ensemble grains de beauté, lesquels sont malins et présentent un danger pour le patient. Pour ce faire, nous construisons sur algorithme de classification d'images distingant les grains de beauté \"malins\" et \"bénins\". \n",
    "\n",
    "NB : cette problématique a constitué l'objet d'un [challenge sur la plateforme Kaggle](https://www.kaggle.com/c/siim-isic-melanoma-classification), auquel ont participé plus de 3 000 équipes. Dans la mesure où nous sommes débutantes en machine learning, notre objectif n'est absolument pas de rivaliser avec tous ces participants : nous ne saurions prétendre concurrencer les gagnants d'un tel challenge en termes d'efficacité du diagnostic. Notre approche serait pluôt : comment, en partant de zéro en machine learning, peut-on essayer de traiter des images médicales pour y poser un diagnotic ?\n",
    "\n",
    "### Plan\n",
    "\n",
    "Notre projet se décompose en trois parties :\n",
    "1. [Récupération et traitement des données](#1.-Récupération-et-traitement-des-données)\n",
    "2. [Analyse de la base](#2.-Analyse-de-la-base)\n",
    "3. [Modélisation et diagnostic](#3.-Modélisation-et-diagnostic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préalable : téléchargement des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indiquez ici le chemin vers le dossier où se trouvent les modules téléchargés sur Github ( Recuperation_des_donnees , rapport, ...  )\n",
    "path_files_modules = r\"C:\\Users\\jeann\\Desktop\\GIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules classiques \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os \n",
    "import random\n",
    "import sys\n",
    "\n",
    "# modules créés pour ce projet \n",
    "sys.path.insert(0, path_files_modules)\n",
    "import Recuperation_des_donnees as RD\n",
    "\n",
    "# pour télécharger les données\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# pour la visualisation des données\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modules de traitement d'images\n",
    "import cv2  # Utiliser pip install opencv-python et redemarrer le noyau si besoin\n",
    "import PIL\n",
    "\n",
    "# modules de traitement du format DICOM\n",
    "import pydicom as dicom  # Utiliser pip install pydicom et redemarrer le noyau si besoin\n",
    "import pydicom.data\n",
    "from pydicom.pixel_data_handlers.util import convert_color_space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Récupération et traitement des données\n",
    "\n",
    "Nous avons choisi d'effectuer ce projet sur le thème de la reconnaissance de mélanomes pour deux raisons : \n",
    "- il s'agit d'une forme de reconnaissance d'images médicales relativement accessible (plus accessible que le traitement d'images médicales comme des scanners ou des IRM par exemple) ;\n",
    "- une large base d'images de grains de beauté est mise à disposition librement par la SIIM (*Society for Imaging Informatics in Medicine*) et l'ISIC (*International Skin Imaging Collaboration*). \n",
    "\n",
    "Le premier enjeu, et pas des moindres, est donc la récupération de ces données et leur traitement pour pouvoir ensuite les exploiter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) téléchargement des données\n",
    "\n",
    "Les données sont disponibles [sur le site de l'ISIC](https://challenge2020.isic-archive.com/). Il s'agit de photos de grains de beauté, chacune étant accompagnée de métadonnées (âge et sexe du patient, partie du corps concerncée, etc.). Ces données sont disponibles sous plusieurs formats : \n",
    "- au format DICOM (*Digital Imaging and Communications in Medecine*) : il s'agit d'un format standard international pour la gestion informatique des données issues de l'imagerie medicale. Chaque fichier .dcm comprend une image et les métadonnées s'y rapportant. \n",
    "- aux formats JPG (pour les images) et CSV (pour les métadonnées) : ces formats sont également proposés par l'ISIC car plus facilement utilisables par le grand public. \n",
    "\n",
    "Nous avons fait le choix d'__utiliser des données au format DICOM__ et de les retraiter nous-mêmes pour les adapter à nos besoins. Nous souhaitons en effet inscrie ce projet dans une perspective médicale, en utilisant donc les données telles qu'elles se présentent en imagerie médicale.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un dossier Projet_Melanomes comprenant : \n",
    "<li> ISIC_2020_Training_Dicom: base de données complete comprenant 33 106 images de grains de beauté avec leurs métadonnées associées</li>\n",
    "<li> Diagnostic : fichier CSV complémentaire à la base ISIC_2020_Training_Dicom nous indiquant le diagnostic de chaque grain de beauté (bénin ou malin) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Répondez par 1 pour 'Oui' et 0 pour 'Non' à ce questionnaire \n",
      "Avez vous déja lancé ce programme ? (le dossier \"Projet_Melanomes\" contenant le fichier Base_complete (ISIC_2020_Training_Dicom dezippé) et le fichier Diagnostic est-il deja créé ?) [1 :\"oui\", 0: \"non\"] 1\n",
      "Insérez le chemin du document 'Projet_Melanomes' (exemple : C:/Users/louis/OneDrive/Bureau/Projet_Melanomes) : D:\\Projet_Melanomes\n"
     ]
    }
   ],
   "source": [
    "RD.Premiere_fonction ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la fin de cette fonction, vous devez avoir : \n",
    "- Le dossier **\" Projet_Melanomes\"** contenant : \n",
    "    - le dossier **\"ISIC_2020_Training_Dicom\"** ( fichier zip téléchargé depuis le site https://challenge2020.isic-archive.com/ )\n",
    "    - le dossier **\"Base_complete\"** ( extraction du fichier ISIC_2020_Training_Dicom) contenant : \n",
    "        - le dossier \"train\" contenant : \n",
    "            - les images au format dicom \n",
    "    - le dossier **\"Dicom_Sample_Test\"**, pour le moment vide, il contiendra les images au format **dicom** de notre base de **test** \n",
    "    - le dossier **\"Dicom_Sample_Train\"**, pour le moment vide, il contiendra les images au format **dicom** de notre base de **train**\n",
    "    - le dossier **\"JPG_Sample_Test\"**, pour le moment vide, il contiendra les images au format **jpeg** de notre base de **test**\n",
    "    - le dossier **\"JPG_Sample_Test_Resize\"**, pour le moment vide, il contiendra les images **réduites** et au format **jpeg** de notre base **test**\n",
    "    - le dossier **\"JPG_Sample_Train\"**, pour le moment vide il contiendra des images au format **jpeg** de notre base **train** \n",
    "    -  le dossier **\"JPG_Sample_Train_Resize\"**, pour le moment vide, il contiendra les images **réduites** au format **jpeg** de notre base **train** \n",
    "    - le fichier .csv **\"Diagnostic\"** téléchargé à partir du site https://challenge2020.isic-archive.com/, rapportant le diagnostic des patients (mélin ou malin) \n",
    "    \n",
    "    \n",
    "Au cours de ce rapport, nous allons ajouter dans le dossier  **\" Projet_Melanomes\"**  :\n",
    "- le fichier **\"Base_complete\"** au format .csv;  dataframe contenant l'ensemble de nos métadonnées \n",
    "- le fichier **\"Sample\"** au format .csv; dataframe contenant l'ensemble des métadonnées de l'échantillon selectionné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dossier Projet_Melanomes créé à partir de la fonction ci-dessus\n",
    "Path_Projet_Melanomes= RD.Path_Projet_Melanomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) traitement des données\n",
    "\n",
    "Maintenant que nous avons téléchargé toutes les données, nous pouvons commencer à les explorer - en particulier regarder comment se présente le format DICOM (format des images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on choisit un fichier DICOM au hasard pour regarder sa structure \n",
    "path_dicom = Path_Projet_Melanomes+'/Base_complete/train'\n",
    "files = os.listdir(path_dicom)\n",
    "i = random.randint(0, len(files))\n",
    "file = files[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des métadonnées associées à l'image\n",
    "filename = pydicom.data.data_manager.get_files(path_dicom, file)[0]\n",
    "ds = pydicom.dcmread(filename)\n",
    "print(ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que les métadonnées sont très nombreuses ; mais seules quelques-unes nous intéressent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image :\", file)\n",
    "print(\"nom du patient :\", ds.PatientName)\n",
    "print(\"âge du patient :\", ds.PatientAge)\n",
    "print(\"sexe du patient :\", ds.PatientSex)\n",
    "print(\"prtie du corps : \", ds.BodyPartExamined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut enfin afficher l'image en tant que telle : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ds.pixel_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que les couleurs de l'image n'ont pas l'air naturelles : il faut changer leur format de couleur pour avoir une aperçu des couleurs \"naturelles\" (format RGB). \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_color = ds.PhotometricInterpretation\n",
    "convert = convert_color_space(ds.pixel_array, initial_color, 'RGB')\n",
    "plt.imshow(convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous venons de le voir, le format DICOM peut être manipulé sur python à l'aide du module \"pydicom\". Cependant, ce format ne nous apparaît pas comme le plus facilement manipulable : nous préférons donc convertir ces données dans un format avec lequel nous sommes plus à l'aise. \n",
    "\n",
    "Deux étapes dans le traitement des données : \n",
    "- l'extraction des métadonnées de chaque fichier DICOM, pour les insérer dans un dataframe. \n",
    "- l'extraction des images de chaque fichier DICOM, pour les enregistrer au format JPG. \n",
    "\n",
    "Pour ce faire, nous avons créé une classe Dataframe dans le module Recuperation_des_donnees nous permettant d'extraire les métadonnées et de changer le format des images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(RD.Dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=RD.Dataframe()\n",
    "# Mise à jour de notre classe en fonction de nos chemins d'accès \n",
    "w.path_base_complete = Path_Projet_Melanomes+'/Base_complete/train'\n",
    "w.path_Diagnostic= Path_Projet_Melanomes+'/Diagnostic.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous traitons maintenant l'ensemble des fichiers DICOM téléchargés pour les insérer dans le dataframe des métadonnées. \n",
    "\n",
    "( __/!\\ attention__ : là encore, la cellule suivant peut prendre plusieurs heures à tourner. Le programe doit en effet passer en revue quelques 33 106 fichiers pour en extraire des données, ce qui peut être un peu long).\n",
    "\n",
    "Afin de limiter le temps de chargement, vous pouvez trouver ce fichier directement en format .csv sur le drive https://drive.google.com/drive/folders/1ByHZayDJD6OiB7g9D3hHsUMFWFZmeBy3?usp=sharing. Merci de l'enregistrer sous le nom **\"Base_complete\"** (format .csv) dans le dossier **\"Projet_Melanomes\"**. Les deux prochaines cellules ne sont pas à faire tourner dans le cas d'un téléchargement sur le Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = w.from_DICOM_to_DF()\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme cette étape de création et de remplissage du datagrame complet est très très longue, nous préférons éviter de devoir la relancer à chaque nouvelle ouverture du projet ; aussi enregistrons-nous ce dataframe au format csv, pour pouvoir ensuite l'ouvrir plus facilement et rapidement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Path_Projet_Melanomes+'/Base_complete.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse de la base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant analyser la base ainsi construite, et la représenter sous forme de graphiques pour se familiariser avec les données. \n",
    "\n",
    "Au préalable, nous téléchargeons les modules nécessaires :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation\n",
    "import seaborn as sns\n",
    "\n",
    "#Tests statistiques \n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous rehchargons la base à partir du CSV précédemment enregistré, pour éviter de devoir relancer le téléchargement de la base à chaque fois. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>patient_age</th>\n",
       "      <th>patient_sex</th>\n",
       "      <th>body_part</th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>UPPER EXTREMITY</td>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_3075186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>LOWER EXTREMITY</td>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_2842074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>45</td>\n",
       "      <td>F</td>\n",
       "      <td>HEAD/NECK</td>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_6890425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>55</td>\n",
       "      <td>F</td>\n",
       "      <td>UPPER EXTREMITY</td>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_8723313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ISIC_0074311</td>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>LOWER EXTREMITY</td>\n",
       "      <td>ISIC_0074311</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_2950485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      image_id  patient_age patient_sex        body_part  \\\n",
       "0           0  ISIC_0015719           40           F  UPPER EXTREMITY   \n",
       "1           1  ISIC_0052212           50           F  LOWER EXTREMITY   \n",
       "2           2  ISIC_0068279           45           F        HEAD/NECK   \n",
       "3           3  ISIC_0074268           55           F  UPPER EXTREMITY   \n",
       "4           4  ISIC_0074311           40           F  LOWER EXTREMITY   \n",
       "\n",
       "     image_name  target  patient_id  \n",
       "0  ISIC_0015719       0  IP_3075186  \n",
       "1  ISIC_0052212       0  IP_2842074  \n",
       "2  ISIC_0068279       0  IP_6890425  \n",
       "3  ISIC_0074268       0  IP_8723313  \n",
       "4  ISIC_0074311       0  IP_2950485  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(Path_Projet_Melanomes+'/Base_complete.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que cette base est volumineuse : 33 126 lignes, c'est-à-dire 33 126 images de mélanomes (le nombre de patient peut être moindre : un même patient peut avoir plusieurs images de ménalomes). \n",
    "\n",
    "Par manque de puissance de calculs, nous ne sommes pas en mesure de traiter une telle base de données. C'est pourquoi nous souhaitons l'échantillonner avant de créer un algorithme de machin learning permettant d'identifier les mélanomes bénins et malins.\n",
    "Comment selectionner notre échantillon ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande permet d'identifier le nombre de valeurs manques. Il n'y en a ici que très peu : seulement 65 patients sur 33 126 ont omis de renseigner leur genre (valeur 'nan') et 49 patients ont renseigné 'X'. \n",
    "Les valeurs non-exploitables de la variable 'Patient_sex' représentent moins de 0,3% de notre échantillon, nous ne ferons donc pas d'étude sur les non réponses, et retirons ces patients de notre base de données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[ df['patient_sex']=='X'].index\n",
    "df.drop(indexNames , inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "# Enregistrement sur notre fichier.csv\n",
    "df.to_csv(Path_Projet_Melanomes+'/Base_complete.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Analyses univariées\n",
    "\n",
    "#### a) L'âge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['patient_age'].hist( facecolor='b', alpha=0.5)\n",
    "plt.title('Age des patients')\n",
    "plt.show()\n",
    "print('Moyenne :', df['patient_age'].mean(), '\\n', 'Ecart type:',  df['patient_age'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Le sexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('patient_sex')['patient_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"patient_sex\"].value_counts().plot(kind='pie' , autopct='%1.1f%%')\n",
    "plt.xlabel(' Parité Hommes/Femmes', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La parité Homme/Femme est respectée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) La partie du corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_part'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=df['body_part'].unique(), y=df['body_part'].value_counts(), palette=\"Reds_r\")\n",
    "plt.xlabel('\\nParties du corps', fontsize=15, color='#c0392b')\n",
    "plt.ylabel(\"Nombre de grains de beauté\\n\", fontsize=15, color='#c0392b')\n",
    "plt.title(\"Localisation des grains de beauté\\n\", fontsize=18, color='#e74c3c')\n",
    "plt.xticks(rotation= 45)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Melanomes bénins et malins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie([sum(df['target']==1),sum(df['target']==0)], labels = ['Malins','Benins'],colors = ['lightcoral','lightskyblue'],autopct='%1.1f%%')\n",
    "plt.title(\"Taux de melanomes benins et malins dans l'échantillon\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nombre d'images :\", df[\"image_id\"].nunique())\n",
    "print(\"nombre de patients :\", df[\"patient_id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a bien moins de patients que d'images (33 012 images pour 2 051 patients) : plusieurs images peuvent donc appartenir à un même patient (environ 16 images par patient en moyenne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = df.groupby(\"patient_id\").sum()[\"target\"]\n",
    "df2 = df.groupby(\"patient_id\").count()[\"image_name\"]\n",
    "\n",
    "pd.concat([df1, df2], axis=1, sort=False).sort_values(by = \"image_name\", ascending = False).head(150)\n",
    "\n",
    "pd.concat([df1, df2], axis=1, sort=False).sort_values(by = \"target\", ascending = False).head(150)\n",
    "\n",
    "pd.concat([df1, df2], axis=1, sort=False).sort_values(by = \"target\", ascending = False).head(20).plot(kind = \"bar\", figsize = (25,5))\n",
    "plt.title(\"nombre de mélanomes bénins et d'images pour les 20 patients ayant le plus de mélanomes bénins\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a jusqu'à 115 images par patient, et jusqu'à 8 mélanomes par patient au sein de la base de donnée. Mais les patients qui ont le plus d'images ne sont pas nécessairement ceux qui ont le plus de mélanomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(\"patient_id\").agg({\"target\" : \"sum\"})\n",
    "print(\"nombre moyen de mélanomes par patient :\", np.average(df1[\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre moyen de mélanomes par patient est surprenamment élevé : 0,28. Nous attentions un chiffre beaucoup plus bas du fait de la faible proportion du mélanomes parmi les images de la base (moins de 2%). Cela est en fait dû au faible nombre de patients au regard du nombre d'images. Si les ménalomes sont très dilués parmi les images, ils le sont beaucoup moins parmi les patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"nombre de patients n'ayant aucun mélanomes :\", len(df1[df1[\"target\"] == 0]))\n",
    "print (\"nombre de patients ayant au moins un mélanome :\", len(df1[df1[\"target\"] > 0]))\n",
    "print(\"proportion de patients ayant au moins un mélanome :\", len(df1[df1[\"target\"] > 0]) / df[\"patient_id\"].nunique() *100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus de 20% des patients de la base ont un mélanome : cette proportion est très très largement supérieure à la proportion de personnes touchées par un mélanome au sein de la population (le taux d'incidence des mélanomes étant d'environ 10 pour 100 000 en France). Cette différence s'explique par le fait que nous disposons d'un échantillon de données médicales sur-représentant volontairement les cas malins, pour apprendre à les diagnostiquer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nombre moyen de mélanomes parmi les patients en ayant au moins un :\", np.average(df1[df1[\"target\"] > 0][\"target\"]))\n",
    "print(\"nombre de patients ayant plusieurs mélanomes :\", len(df1[df1[\"target\"] > 1]))\n",
    "print(\"proportion de patients ayant plusieurs mélanomes parmi les patients malades :\", len(df1[df1[\"target\"] > 1]) / len(df1[df1[\"target\"] > 0])*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parmi les patients malades (i.e. ayant au moins un mélanome), plus d'un quart présente plusieurs mélanomes (jusqu'à 8, comme vu plus haut). Les mélanomes, en faible nombre dans la base au regard du nombre d'image, sont donc en fait concentrés sur une relativement petite part des patients, la majorité n'ayant que des grains de beauté bénins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Analyses bivariées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Les mélanomes et l'âge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.kdeplot(df[\"patient_age\"][df.target == 1], color=\"darkturquoise\", shade=True)\n",
    "sns.kdeplot(df[\"patient_age\"][df.target == 0], color=\"lightcoral\", shade=True)\n",
    "plt.legend(['Melanome malin', 'Melanome bénin'])\n",
    "plt.title(\"Fonctions de densité : répartition de l'age pour les mélanomes malins et bénins\")\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribution des mélanomes malins est décalée sur la droite par rapport à celle des bénins. Cela signifie que les personnes agées sont plus touchées par les mélanomes malins que les personnes jeunes. Les articles scientifiques confirment cette hypothèse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Melanomes et sexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby('patient_sex')['target'].sum() / df.groupby('patient_sex')['patient_id'].count() *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='patient_sex', y='target', data=df, color=\"aquamarine\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après cet échantillon les hommes sont plus touchés par les mélanomes malins que les femmes. \n",
    "1,37% des femmes de l'échantillon sont porteuses d'un mélanome bénins contre 2,13% des hommes.\n",
    "Les articles scientifiques ne confirment pas cette hypothèse. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.ttest_ind(df[\"target\"][df.patient_sex == 'F'],df[\"target\"][df.patient_sex == 'M']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après les résultats du T-test, les populations féminines et masculines sont significativement différentes en ce qui concerne les mélanomes (p-value < 0,001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Mélanomes et partie du corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby('body_part')['target'].sum() / df.groupby('body_part')['patient_id'].count() *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df['target'], y=df['body_part'],palette='Blues_d', orient='h',  order=[\"HEAD/NECK\", \"ORAL/GENITAL\",\"UPPER EXTREMITY\",\"SKIN\", \"TORSO\", \"LOWER EXTREMITY\", \"PALMS/SOLES\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le taux de mélanomes malins est plus élevé sur les parties du corps : \n",
    "- tête et cou\n",
    "- orales et génitales \n",
    "- les membres supérieurs\n",
    "      \n",
    "Selon les articles scientifiques, les zones les plus exposées aux mélanomes malins sont les parties les plus exposées au soleil.\n",
    "Il est donc raisonnable d'identifier la tête, le cou et les membres supérieurs dans les parties du corps les plus touchées par les mélanomes malins. \n",
    "Dans ce sens, cela est étonnant d'identifier les zones orales et génitales comme zones à risque. Cependant, dans notre échantillon, la catégorie \"Oral/Genital\" est la moins représentée des parties du corps; il n'y a que 124 images. Ce qui est très faible par rapport à notre base totale. L'échantillon est probablement peu représentatif de la population total pour cette partie du corps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.kruskal(df[\"target\"][df.body_part == 'HEAD/NECK'],df[\"target\"][df.body_part == 'ORAL/GENITAL'],df[\"target\"][df.body_part == 'SKIN'],df[\"target\"][df.body_part == \"UPPER EXTREMITY\"],df[\"target\"][df.body_part == \"TORSO\"],df[\"target\"][df.body_part == \"PALMS/SOLES\"],df[\"target\"][df.body_part == \"LOWER EXTREMITY\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après le test de Kruskal Wallis les populations de mélanomes sont différentes en fonction des parties du corps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Régression linéaire multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression linéaire multiple permet de créer des modéles permetant d'expliquer une variable Y à partir des variables explicatives X. Nous allons essayer d'expliquer la variable \"targer\" à partir de l'age du patient, son sexe et la partie du corps sur laquelle se trouve le grain de beauté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=pd.get_dummies(df, columns=[\"patient_sex\",\"body_part\"])\n",
    "\n",
    "# On retire la variable patient_sex_F, qui est parfaitement colinéaire à patient_sex_M\n",
    "training.drop('patient_sex_F', axis=1, inplace=True) \n",
    "final_train = training\n",
    "final_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_train[[\"patient_age\", \"patient_sex_M\", \"body_part_HEAD/NECK\", \"body_part_LOWER EXTREMITY\", \n",
    "              \"body_part_ORAL/GENITAL\",\"body_part_PALMS/SOLES\",\"body_part_SKIN\", \"body_part_TORSO\", \"body_part_UPPER EXTREMITY\"]]\n",
    "X = sm.add_constant(X) # une autre façons d'ajouter une constante\n",
    "y = final_train[\"target\"]\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_features = [\"patient_age\", \"patient_sex_M\", \"body_part_HEAD/NECK\", \"body_part_LOWER EXTREMITY\", \n",
    "              \"body_part_ORAL/GENITAL\",\"body_part_PALMS/SOLES\",\"body_part_SKIN\", \"body_part_TORSO\", \"body_part_UPPER EXTREMITY\"]\n",
    "X = final_train[Selected_features]\n",
    "\n",
    "plt.subplots(figsize=(10, 7))\n",
    "sns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modéle n'est pas convainquant, les coefficients sont très faibles, les variables age, parties du corps et sexe expliquent à hauteur de 0,9% les mélanomes malin (R²=0,009).\n",
    "Il n'est pas possible d'expliquer convenablement les mélanomes malins à partir de ces variables explicatives.\n",
    "Il est donc primordial d'analyser les images de mélanomes afin de fournir un modèle prédictif acceptable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression logistique est une technique prédictive. Elle vise à construire un modèle permettant de prédire / expliquer les valeurs prises par une variable cible qualitative (le plus souvent binaire, on parle alors de régression logistique binaire) à partir d’un ensemble de variables explicatives quantitatives ou qualitatives (un codage est nécessaire dans ce cas).\n",
    "Nous voulons ici expliquer la variable 'target' (binaire) en fonction des variables 'patient_age'(quantitative), 'patient_sex'(qualitative), 'body_part'(qualitative). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On divise la base en deux : 80% dans la base de training et 20% dans la base test. \n",
    "X_app,X_test,y_app,y_test = model_selection.train_test_split(X,y,test_size = 6500,random_state=0) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\")\n",
    "# Construction du modéle predictif :  \n",
    "modele = lr.fit(X_app,y_app)\n",
    "\n",
    "#Prediction sur l'échantillon test\n",
    "y_pred = modele.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrice de confusion : confrontation entre les Y observés sur l’échantillon test et la prédiction\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.heatmap(cm, annot=True, \n",
    "           annot_kws={\"size\": 10}, \n",
    "           xticklabels=('Prédiction : Bénin', 'Prédiction : Malin'), \n",
    "           yticklabels= ('Benin', 'Malin'), ax = ax)\n",
    "ax.set_title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = metrics.accuracy_score(y_test,y_pred)\n",
    "print('Le taux de succés est de ',acc *100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourrait croire que ce modéle de classification est efficace ( il a un taux de succés de 98%). Cependant, cela est dû à la proportion de bénins et de malins de notre base. En effet, le taux de mélanomes malins est très faible ( moins de 2%), notre modéle, en prédisant toujours bénin, arrive donc à limiter ses erreurs. Ce modéle n'est pas satisfaisant pour différencier les mélanomes malins des mélanomes bénins. Il apparait primordial d'utiliser les images afin de rendre une prédiction acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modélisation et diagnostic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import des packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module créé pour ce projet \n",
    "import Echantillonnage\n",
    "\n",
    "# package machine learning\n",
    "import sklearn \n",
    "\n",
    "#package TensorFlow (pour le CNN)\n",
    "import tensorflow as tf    \n",
    "\n",
    "import shutil \n",
    "import pydicom\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Création des bases train et test\n",
    "\n",
    "On veut constituer une base d'entraînement (train) et une base de test à partir de l'ensemble des images (plus de 33 000). \n",
    "Cette base de donnée est représentée par le Dataframe \"df\" importé précedemment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(Path_Projet_Melanomes+'/Base_complete.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du taux de mélanomes malins au sein de cette base :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sum(df['target']==1) / len(df.index)\n",
    "print(\"taux de malignité des mélanomes :\", t*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avions déjà vu, le taux de malignité des mélanomes est très faible (1,76%).\n",
    "\n",
    "Aussi, dans un premier temps, pour faciliter la construction de premiers modèles, nous construisons une base réduite avec un taux de malignité supérieur. Nous réduisons ainsi le nombre total d'images, ce qui nous permettra aussi de construire des modèles plus économes en mémoire vive pour l'instant. C'est à partir de cette base réduite que nous constitueront les bases test et train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) échantillonnage \n",
    "\n",
    "Pour ce faire, on commence par définir une fonction d'échantillonnage simple dans notre module Echantillonnage : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Echantillonnage.simple_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous choisissons d'extraire un échantillon de 1 000 images dont 50% représentent des grains de beauté malins (mélanomes) ; ces paramètres pourront être amenés à varier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "size = 1000\n",
    "malignancy_rate = 0.5\n",
    "df_sample = Echantillonnage.simple_sampling(df, size, malignancy_rate)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde l'échantillon ainsi construit dans un CSV (au cas où)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-78543057c4fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# IIIIIIIIIIIIIIIIIIICCCCCCCCCCCCCCCCCCIIIIIIIIIIIIIIIIIIIII\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath_Projet_Melanomes\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/Sample.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_sample' is not defined"
     ]
    }
   ],
   "source": [
    "df_sample.to_csv(Path_Projet_Melanomes+'/Sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) train / test split\n",
    "\n",
    "Pour diviser cet échantillon en train et test, on s'appuie sur le package scikit-learn. \n",
    "\n",
    "Pour l'instant, on choisit arbitrairement d'allouer 20% de notre échantillon au test et 80% au test - proportions que l'on pourra changer par la suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample[\"image_id\"]\n",
    "Y = df_sample[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Définition des classes et import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "définition des classes (deux seulement : bénin et malin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['benin', 'malin']\n",
    "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "nb_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benin': 0, 'malin': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 150\n",
    "# peut être modifié en adaptant les paramètres du CNN en conséquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>image_id</th>\n",
       "      <th>patient_age</th>\n",
       "      <th>patient_sex</th>\n",
       "      <th>body_part</th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>ISIC_0149568</td>\n",
       "      <td>55</td>\n",
       "      <td>F</td>\n",
       "      <td>UPPER EXTREMITY</td>\n",
       "      <td>ISIC_0149568</td>\n",
       "      <td>1</td>\n",
       "      <td>IP_0962375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "      <td>234</td>\n",
       "      <td>ISIC_0188432</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>UPPER EXTREMITY</td>\n",
       "      <td>ISIC_0188432</td>\n",
       "      <td>1</td>\n",
       "      <td>IP_0135517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312</td>\n",
       "      <td>313</td>\n",
       "      <td>ISIC_0207268</td>\n",
       "      <td>55</td>\n",
       "      <td>M</td>\n",
       "      <td>TORSO</td>\n",
       "      <td>ISIC_0207268</td>\n",
       "      <td>1</td>\n",
       "      <td>IP_7735373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>396</td>\n",
       "      <td>398</td>\n",
       "      <td>ISIC_0232101</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>TORSO</td>\n",
       "      <td>ISIC_0232101</td>\n",
       "      <td>1</td>\n",
       "      <td>IP_8349964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>472</td>\n",
       "      <td>474</td>\n",
       "      <td>ISIC_0250839</td>\n",
       "      <td>75</td>\n",
       "      <td>M</td>\n",
       "      <td>HEAD/NECK</td>\n",
       "      <td>ISIC_0250839</td>\n",
       "      <td>1</td>\n",
       "      <td>IP_6234053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1      image_id  patient_age patient_sex  \\\n",
       "0          89            90  ISIC_0149568           55           F   \n",
       "1         233           234  ISIC_0188432           50           F   \n",
       "2         312           313  ISIC_0207268           55           M   \n",
       "3         396           398  ISIC_0232101           60           M   \n",
       "4         472           474  ISIC_0250839           75           M   \n",
       "\n",
       "         body_part    image_name  target  patient_id  \n",
       "0  UPPER EXTREMITY  ISIC_0149568       1  IP_0962375  \n",
       "1  UPPER EXTREMITY  ISIC_0188432       1  IP_0135517  \n",
       "2            TORSO  ISIC_0207268       1  IP_7735373  \n",
       "3            TORSO  ISIC_0232101       1  IP_8349964  \n",
       "4        HEAD/NECK  ISIC_0250839       1  IP_6234053  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample=pd.read_csv(Path_Projet_Melanomes+'/sample.csv')\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un dossier avec les images DICOM de la base train\n",
    "path_dicom_sample_train = Path_Projet_Melanomes+'/Dicom_Sample_Train'  \n",
    "\n",
    "for file in X_train :\n",
    "    shutil.copy(w.path_base_complete + '/' + file +'.dcm', path_dicom_sample_train + '/' + file +'.dcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un dossier avec les images DICOM de la base test\n",
    "path_dicom_sample_test =  Path_Projet_Melanomes+'/Dicom_Sample_Test' \n",
    "\n",
    "for file in X_test :\n",
    "    shutil.copy(w.path_base_complete + '/' + file +'.dcm', path_dicom_sample_test + '/' + file +'.dcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion des images train en jpg - attention cette fonction est longue à exécuter (plusieurs dizaines de minutes)\n",
    "w.path_base_complete = path_dicom_sample_train\n",
    "w.path_jpg_RGB=Path_Projet_Melanomes+'/JPG_Sample_Train' \n",
    "w.convert_to_JPG_RGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion des images test en jpg - attention cette fonction est longue à exécuter (plusieurs dizaines de minutes)\n",
    "w.path_base_complete = path_dicom_sample_test\n",
    "w.path_jpg_RGB=Path_Projet_Melanomes+'/JPG_Sample_Test' \n",
    "w.convert_to_JPG_RGB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On redimensionne les images du dossier *Sample_Test* et *Sample_Train*, et on enregistre les images ainsi redimensionnées dans les dossiers *Sample_Test_Resize* et *Sample_Train_Resize* :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le dossier Train : (Quelques minutes)\n",
    "w.path_jpg_RGB = Path_Projet_Melanomes+'/JPG_Sample_Train'\n",
    "w.path_jpg_Resize = Path_Projet_Melanomes+'/JPG_Sample_Train_Resize'\n",
    "w.redimensionner((image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le dossier Test : (Quelques minutes)\n",
    "w.path_jpg_RGB = Path_Projet_Melanomes+'/JPG_Sample_Test'\n",
    "w.path_jpg_Resize = Path_Projet_Melanomes+'/JPG_Sample_Test_Resize'\n",
    "w.redimensionner((image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut ensuite charger les données pour pourvoir les insérer dans le modèle (attention, cette cellule prend un peu de temps à être exécutée, de l'ordre de quelques minutes) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [01:11, 11.18it/s]\n",
      "200it [00:17, 11.45it/s]\n"
     ]
    }
   ],
   "source": [
    "path_jpg_train = Path_Projet_Melanomes+'/JPG_Sample_Train_Resize' \n",
    "path_jpg_test = Path_Projet_Melanomes+'/JPG_Sample_Test_Resize' \n",
    "    \n",
    "meta_data = df_sample\n",
    "meta_data = meta_data.set_index(\"image_id\")\n",
    "datasets = [path_jpg_train, path_jpg_test]\n",
    "\n",
    "output = []\n",
    "\n",
    "#itération sur chaque dataset (train puis test)\n",
    "for dataset in datasets :\n",
    "\n",
    "    files = os.listdir(dataset)\n",
    "    images = np.zeros((len(files), image_size, image_size, 3), dtype=np.float32)\n",
    "    labels = np.zeros(len(files))\n",
    "\n",
    "    # itération sur chaque image du dataset\n",
    "    for i, file in tqdm(enumerate(files)):\n",
    "        \n",
    "        # obtention du le chemin de l'image\n",
    "        img_path = os.path.join(dataset, file)\n",
    "\n",
    "        # obtention du label de l'image\n",
    "        label = meta_data.loc[file[0:12]].target\n",
    "\n",
    "        # ouverture de l'image et retraitement de l'image (changement de couleurs et réduction de taille)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "\n",
    "        # ajout de l'image et de son label à la liste de résultats\n",
    "        images[i, :] = (np.array(image) / 255).astype(np.float32)\n",
    "        labels[i] = label\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    output.append((images, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on renomme les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels),(test_images, test_labels) = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on mélanges les données de la base test pour ne plus avoir les malins en haut et les bénins en bas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = sklearn.utils.shuffle(train_images, train_labels, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Visualisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Exploration de l'échantillon de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = train_labels.shape[0]\n",
    "n_test = test_labels.shape[0]\n",
    "\n",
    "print (\"nombre d'images dans la base train :\", n_train)\n",
    "print (\"nombre d'images dans la base test :\", n_test)\n",
    "print (\"taille de chaque image :\", image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation d'exemples d'images de la base, pour en avoir un aperçu : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_examples(class_names, images, labels):\n",
    "   \n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    fig.suptitle(\"Exemples d'images de la base train\", fontsize=20)\n",
    "    for i in range(15):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(images[i])\n",
    "        plt.xlabel(class_names[labels[i].astype(np.integer)])\n",
    "    plt.show()\n",
    "\n",
    "display_examples(class_names, train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Modèle de CNN\n",
    "\n",
    "Un réseau de neurones convolutifs est un système composé de neurones, généralement répartis en plusieurs couches connectées entre elles. Nous l'utilison ici pour résoudre un problème de classification. Le réseau calcule à partir de l'entrée une probabilité pour chaque classe. \n",
    "Chaque couche reçoit en entrée des données et les renvoie transformée. Pour cela, elle calcule une combinaison linéaire puis applique éventuellement une fonction non-linéaire, appelée fonction d'activation. Les coefficients de la combinaison linéaire définissent les paramètres (ou poids) de la couche.\n",
    "Un réseau de neurones est construit en empilant les couches : la sortie d'une couche correspond à l'entrée de la suivante.\n",
    "La dernière couche calcule les probabilités finales en utilisant pour fonction d'activation la fonction logistique (pour une classification binaire).\n",
    "Une fonction de perte (loss function) est associée à la couche finale pour calculer l'erreur de classification.   \n",
    "N'ayant pas encore fait de cours de Machine Learning à l'ENSAE, nous nous sommes basées sur les ressources suivantes :    \n",
    "<li> cours openclassrooms Classez et segmentez des données visuelles : https://openclassrooms.com/fr/courses/4470531-classez-et-segmentez-des-donnees-visuelles</li>\n",
    "<li> https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les CNN réalisent eux-mêmes le travail fastidieux d'extraction et de description de features, cela constitue une des forces des réseaux de neurones convolutifs : plus besoin d'implémenter un algorithme d'extraction \"à la main\", comme SIFT ou Harris-Stephens. \n",
    "Comme nous l'avons vu précédemment, un reseau de neurones est constitué de plusieurs couches. Il existe quatre types de couches pour un réseau de neurones convolutif : \n",
    "<p>\n",
    "<li>la couche de convolution,</li> \n",
    "<li> la couche de pooling, </li> \n",
    "<li> la couche de correction ReLU, </li> \n",
    "<li> la couche fully-connected.</li> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Premier modèle simple :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (5, 5), activation = 'relu', input_shape = (150, 150, 3)), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),  \n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Explications du modèle : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La couche de convolution** constitue toujours au moins la première couche d'un CNN.Son but est de repérer la présence d'un ensemble de features dans les images reçues en entrée. La couche de convolution reçoit donc en entrée plusieurs images, et calcule la convolution de chacune d'entre elles avec chaque filtre. Les filtres correspondent exactement aux features que l'on souhaite retrouver dans les images. \n",
    "\n",
    "\n",
    "*tensorflow.keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)*\n",
    "\n",
    "   \n",
    "<p>\n",
    "<li> Conv2D : pour les images, Conv1D: dimension 1, Conv3D: pour les volumes </li> \n",
    "<li> Filters (32) : Nombre de filtres a apprendre : commencer par un nombre de filtre assez faible (32) et augmenter dans les couches suivantes dans l'idéal des multiples de 2, on peut apprendre dans les premières couches 32, 64, 128 filtres puis aller jusqu'à 256, 512, 1024 dans les couches plus pronfondes. </li> \n",
    "<li> kernel_size (5,5) : dimension du kernel, si on a des images jusqu'à 128x128 on va utiliser 3x3. Au dessus on utilisera 5x5 ou 7x7. Il faudra dans les couches suivantes se reduire à du 3x3. </li> \n",
    "<li> strides : pas avec lequel on va analyser l'image pixel par pixel. La valeur par défaut est (1, 1); cependant, on peut parfois l'augmenter à (2, 2) pour aider à réduire la taille du volume de sortie.  </li> \n",
    "<li> Padding : Il peut prendre deux paramétres \"valide\" ou \"same\". Avec le paramètre valide, le volume d'entrée n'est pas rempli de zéro et les dimensions spatiales peuvent être réduites via l'application naturelle de la convolution. On preferera reduire les dimensions spaciales avec le max pooling ou la strided convolution. On notera padding = 'same' pour la majorité de nos couches. </li> \n",
    "<li> data_format: Height, Width, Depth </li>\n",
    "<li> dilation_rate : On utilise ce parametre de dilatation quand on travaille avec des images de plus haute résolution (où les détails sont importants) ou quand on construit un réseau avec moins de paramètres. </li>\n",
    "<li> activation : Nom de la fonction d'activation que l'on souhaite appliquer après avoir effectué la convolution. Elle constitue la couche d'activation. On utilisera ici la fonction 'relu' qui est très utilisée dans les CNN. Elle permet un apprentissage plus rapide de notre modéle.  </li>\n",
    "<li> use_bias : Contrôle si un vecteur de biais est ajouté à la couche de convolution. Recommandé de garder le biais sauf dans des cas particuliers.</li>\n",
    "<li> kernel_initializer :contrôle la méthode d'initialisation utilisée pour initialiser toutes les valeurs avant d'entraîner réellement le réseau. Recommandé de ne pas y toucher sauf si réseau très profond</li>\n",
    "<li> bias_initializer : contrôle la façon dont le vecteur de biais est initialisé avant le début de l'entraînement. Recommandé de ne pas y toucher</li>\n",
    "<li> kernel_regularizer, bias_regularizer, activity_regularizer : controle le type de regularisation. Recommandé de ne pas changer les valeurs par défaut.  </li>\n",
    "<li> kernel_constraint,  bias_constraint : Ces paramètres permettent d'imposer des contraintes sur la couche Conv2D (ex : non-négativité, la normalisation d'unité et la normalisation min-max ... ). Il est recommandé de laisser les valeur par defaut.</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La couche de pooling :** Cette couche est souvent placé entre deux couches de convolution : elle reçoit en entrée plusieurs feature maps, et applique à chacune d'entre elles l'opération de pooling. L'opération de pooling consiste à réduire la taille des images, tout en préservant leurs caractéristiques importantes. La couche de pooling permet de réduire le nombre de paramètres et de calculs dans le réseau. On améliore ainsi l'efficacité du réseau et on évite le sur-apprentissage.\n",
    "\n",
    "\n",
    "*tf.keras.layers.MaxPooling2D ( pool_size=(2, 2), strides=None, padding=\"valid\", data_format=None)*\n",
    "\n",
    "<p>\n",
    "<li> pool_size : facteur de réduction d'échelle (vertical, horizontal). (2, 2) réduira de moitié l'entrée dans les deux dimensions spatiales.</li>\n",
    "    <li> Strides,  Padding et data_format ont la même signification que dans la couche de convolution. S'ils ne sont pas précisés dans la fonction de MaxPooling2D, ils prendont les mêmes valeurs que dans la couche de convolution. </li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La couche de correction :** \n",
    "\n",
    "La couche de correction ReLU remplace toutes les valeurs négatives reçues en entrées par des zéros. Elle joue le rôle de fonction d'activation. Elle est appliquée dans la couche de convolution de notre modèle :  activation = 'relu'. La fonction 'relu' est très utilisée dans les CNN. Elle permet un apprentissage plus rapide de notre modéle.\n",
    "\n",
    "Ces deux commandes sont les mêmes :\n",
    "\n",
    " - model.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 3))\n",
    " \n",
    " - model.add(layers.Conv2D(32, (3, 3), input_shape = (150, 150, 3))       \n",
    " model.add(layers.Activation(activations.relu)) </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La couche fully-connected**\n",
    "\n",
    "La couche fully-connected constitue toujours la dernière couche d'un réseau de neurones, convolutif ou non – elle n'est donc pas caractéristique d'un CNN. \n",
    "Ce type de couche reçoit un vecteur en entrée et produit un nouveau vecteur en sortie. Pour cela, elle applique une combinaison linéaire puis éventuellement une fonction d'activation aux valeurs reçues en entrée.\n",
    "La dernière couche fully-connected permet de classifier l'image en entrée du réseau : ici elle renvoie un vecteur de taille 2 ( pour une classification binaire). Chaque élément du vecteur indique la probabilité pour l'image en entrée d'appartenir à une classe (malin ou bénin). \n",
    "\n",
    "La fonction flatten : Elle permet de convertir des matrices 3D en vecteur 1D.\n",
    "\n",
    "La fonciton Dense :     \n",
    "*tf.keras.layers.Dense( units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)*\n",
    "<p>\n",
    "    <li> Units :Entier positif précisant la taille du vecteur en sortie</li>\n",
    "    <li> Activation : indique si une correction ReLU ou softmax est effectuée juste après la couche fully-connected </li>\n",
    "    <li> Les autres paramétres ont la même description que dans la couche de convolution </li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compilation du modéle :**\n",
    "\n",
    "Avant d’entraîner notre modèle, nous devons configurer le processus d’apprentissage en appelant la méthode compile() de Keras.\n",
    "\n",
    "*compile( optimizer, loss = None, metrics = None, loss_weights = None, sample_weight_mode = None, weighted_metrics = None, target_tensors = None )*\n",
    "\n",
    "<p>\n",
    "    <li>optimizer : optimise les poids d'entrée en comparant la fonction de prédiction et de perte. Keras fournit plusieurs optimiseurs sous forme de module. Pour un probleme de classification binaire on utilisera 'rmsprop'</li>\n",
    "\n",
    "<li>loss : Il s’agit de la fonction de coût que le modèle va utiliser pour minimiser les erreurs. On utilisera loss='binary_crossentropy' pour une classification binaire,</li>\n",
    "\n",
    "<li> metrics : En machine learning, les métriques sont utilisées pour évaluer les performances du modèle. Il est similaire à la fonction de perte, mais n'est pas utilisé dans le processus de formation. Keras fournit plusieurs métriques sous forme de module. Dans le cas d'un probleme de classification on utilise metrics = ['accuracy']</li>\n",
    "\n",
    "<li> loss_weights, sample_weight_mode, weighted_metrics, target_tensors : Parametres de pondération facultatifs. Nous garderons ces parametres à leur valeur par défaut.   </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, batch_size=128, epochs=20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de la courbe accuracy au cours des époques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "legend = plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de la courbe loss au cours des époques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "plt.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "legend = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation de la matrice de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)     \n",
    "pred_labels = np.argmax(predictions, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = sklearn.metrics.confusion_matrix(test_labels, pred_labels)\n",
    "ax = plt.axes()\n",
    "sns.heatmap(CM, annot=True, \n",
    "           annot_kws={\"size\": 10}, \n",
    "           xticklabels=class_names, \n",
    "           yticklabels=class_names, ax = ax)\n",
    "ax.set_title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Améliorations du modéle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous souhaitons améliorer la performance de notre modèle. Pour celà, nous pouvons modifier plusieurs aspects de notre réseau de neurones : \n",
    "- Les couches : nous pouvons en ajouter ou en supprimer. Nos images sont assez complexes (dimension 150 x 150, images de couleurs... ), c'est pourquoi nous avons besoin de plusieurs couches.\n",
    "- Ajouter des fonctions : \n",
    "    - Dropout(p) : Cette fonction permet de limiter le sur-apprentissage en abandonnant avec probabilité p certains neurones. \n",
    "    - La fonction BatchNormalization() : elle est utilisée dans de nombreux réseaux de neurones afin d'élever le taux d'apprentissage des modèles en réduisant la sensibilité des réseaux de neurones aux poids de départ. Malgrè plusieurs tentatives, cette fonction n'a pas permis l'amélioration de notre modèle, elle ne sera donc pas ajouter dans la suite de ce rapport.   \n",
    "- Les hypers paramétres : Beaucoup d'hypers paramétres sont présents dans la création du modéle ; le nombre de filtres, les fonctions d'activation, la taille du vecteur de sortie de la fonction Dense (units), le paramétre p de la fonction Dropout(p), le learning rate... Ces hypers paramétres sont modifiables. Nous allons créer une fonction permettant de tester différents hypers paramétres afin de récupérer les paramétres rendant notre modéle optimal. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (Conv2D, Dense,Dropout, Flatten,MaxPooling2D)\n",
    "\n",
    "from hyperopt import hp # pour une première utilisation : pip install hyperopt\n",
    "import kerastuner as kt # pour une première utilisation :  pip install keras-tuner\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from kerastuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On indique les hypers parametres que nous voulons tester ainsi que les choix possibles : \n",
    "\n",
    "# Le nombre de filtres \n",
    "filters=hp.Choice('num_filters', [32,64,128,256])\n",
    "\n",
    "# La taille du vecteur de sortie et la fonction d'activation \n",
    "Dense(units=hp.Int('units', 32, 512, step=32), activation=hp.Choice('dense_activation',['relu','tanh','sigmoid']))\n",
    "\n",
    "# Le learning rate \n",
    "hp.Float('learning_rate',min_value=1e-5, max_value=1e-2, sampling='LOG',default=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "\n",
    "INPUT_SHAPE = (150,150,3)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Création de la classe CNNHyperModel permettant de modifier les parametres souhaités\n",
    "\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add( Conv2D( filters=16, kernel_size=5,activation=\"relu\",input_shape=self.input_shape,))\n",
    "        model.add(Conv2D(filters=hp.Choice(\"num_filters_1\", values=[32,16], default=32,),activation=\"relu\",kernel_size=3,))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Dropout(rate=hp.Float(\"dropout_1\", min_value=0.0, max_value=0.5, default=0.25, step=0.05,)))\n",
    "        \n",
    "        model.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "        model.add(Conv2D(filters=hp.Choice(\"num_filters_2\", values=[32, 64], default=64,),activation=\"relu\",kernel_size=3,))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Dropout(rate=hp.Float(\"dropout_2\", min_value=0.0, max_value=0.5, default=0.25, step=0.05,)))\n",
    "        \n",
    "        model.add(Conv2D(filters=64, kernel_size=3, activation=\"relu\"))\n",
    "        model.add(Conv2D(filters=hp.Choice(\"num_filters_3\", values=[64,128], default=128,),activation=\"relu\",kernel_size=3,))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Dropout(rate=hp.Float(\"dropout_3\", min_value=0.0, max_value=0.5, default=0.25, step=0.05,)))\n",
    "        \n",
    "        model.add(Conv2D(filters=128, kernel_size=3, activation=\"relu\"))\n",
    "        model.add(Conv2D(filters=hp.Choice(\"num_filters_4\", values=[128,256], default=256,),activation=\"relu\",kernel_size=3,))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Dropout(rate=hp.Float(\"dropout_4\", min_value=0.0, max_value=0.5, default=0.25, step=0.05,)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=hp.Int(\"units\", min_value=32, max_value=512, step=32, default=128),activation=hp.Choice('dense_activation',\n",
    "                    values=['relu', 'tanh', 'sigmoid'],\n",
    "                    default='relu',),))\n",
    "        model.add(Dropout(rate=hp.Float(\"dropout_5\", min_value=0.0, max_value=0.5, default=0.25, step=0.05 )))\n",
    "        model.add(Dense(self.num_classes, activation=\"softmax\"))\n",
    "\n",
    "        model.compile(optimizer=keras.optimizers.Adam(hp.Float(\"learning_rate\",min_value=1e-4,max_value=1e-2,sampling=\"LOG\",default=1e-3,)\n",
    "            ),loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"],)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "hypermodel = CNNHyperModel(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "                 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJOUTER DESCRIPTION\n",
    "HYPERBAND_MAX_EPOCHS = 40\n",
    "MAX_TRIALS = 20\n",
    "EXECUTION_PER_TRIAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED= 1\n",
    "from kerastuner.tuners import RandomSearch\n",
    "hypermodel=CNNHyperModel (input_shape = INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "\n",
    "tuner = RandomSearch(hypermodel, objective='val_accuracy', seed = SEED, max_trials=MAX_TRIALS, executions_per_trial=EXECUTION_PER_TRIAL, directory = 'random_search', project_name='CNN_melanome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hyperband\\CNN_melanome\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from hyperband\\CNN_melanome\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "tuner = Hyperband(hypermodel, max_epochs= HYPERBAND_MAX_EPOCHS, objective = 'val_accuracy', seed = SEED, executions_per_trial=EXECUTION_PER_TRIAL, directory='hyperband', project_name='CNN_melanome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 12\n",
      "num_filters_1 (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32, 16], 'ordered': True}\n",
      "dropout_1 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "num_filters_2 (Choice)\n",
      "{'default': 64, 'conditions': [], 'values': [32, 64], 'ordered': True}\n",
      "dropout_2 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "num_filters_3 (Choice)\n",
      "{'default': 128, 'conditions': [], 'values': [64, 128], 'ordered': True}\n",
      "dropout_3 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "num_filters_4 (Choice)\n",
      "{'default': 256, 'conditions': [], 'values': [128, 256], 'ordered': True}\n",
      "dropout_4 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "units (Int)\n",
      "{'default': 128, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "dense_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
      "dropout_5 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 86 Complete [00h 31m 05s]\n",
      "val_accuracy: 0.5875000059604645\n",
      "\n",
      "Best val_accuracy So Far: 0.7749999761581421\n",
      "Total elapsed time: 10h 58m 18s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Attention cette cellule peut mettre plusieurs heures à aboutir. \n",
    "# En effet, l'algorithme va tester toutes les combinaisons possibles pour les hypers parametres \n",
    "N_EPOCH_SEARCH = 40\n",
    "tuner.search(train_images, train_labels, epochs = N_EPOCH_SEARCH, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hyperband\\CNN_melanome\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.05\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.15000000000000002\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.2\n",
      "num_filters_4: 256\n",
      "dropout_4: 0.15000000000000002\n",
      "units: 480\n",
      "dense_activation: tanh\n",
      "dropout_5: 0.30000000000000004\n",
      "learning_rate: 0.00011289813322780023\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 14\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 5782f67da6d7a96af5b6454a6754dfb3\n",
      "Score: 0.7749999761581421\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.15000000000000002\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.05\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.35000000000000003\n",
      "num_filters_4: 128\n",
      "dropout_4: 0.35000000000000003\n",
      "units: 320\n",
      "dense_activation: relu\n",
      "dropout_5: 0.35000000000000003\n",
      "learning_rate: 0.0001077121672213119\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 14\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: eaf46eef6b0202e5415f6f4e467f9840\n",
      "Score: 0.75\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.15000000000000002\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.0\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.25\n",
      "num_filters_4: 128\n",
      "dropout_4: 0.1\n",
      "units: 192\n",
      "dense_activation: tanh\n",
      "dropout_5: 0.0\n",
      "learning_rate: 0.00023899521618106246\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 14\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 2394a72e64bc9b52e0a6bff07510e4f4\n",
      "Score: 0.7437499761581421\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.15000000000000002\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.05\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.35000000000000003\n",
      "num_filters_4: 128\n",
      "dropout_4: 0.35000000000000003\n",
      "units: 320\n",
      "dense_activation: relu\n",
      "dropout_5: 0.35000000000000003\n",
      "learning_rate: 0.0001077121672213119\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.737500011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 32\n",
      "dropout_1: 0.05\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.30000000000000004\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.5\n",
      "num_filters_4: 256\n",
      "dropout_4: 0.1\n",
      "units: 288\n",
      "dense_activation: relu\n",
      "dropout_5: 0.05\n",
      "learning_rate: 0.0001029562160619873\n",
      "tuner/epochs: 40\n",
      "tuner/initial_epoch: 14\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 14a51576d53dca0edbf394bd8157f43f\n",
      "Score: 0.7374999821186066\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.05\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.15000000000000002\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.2\n",
      "num_filters_4: 256\n",
      "dropout_4: 0.15000000000000002\n",
      "units: 480\n",
      "dense_activation: tanh\n",
      "dropout_5: 0.30000000000000004\n",
      "learning_rate: 0.00011289813322780023\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 8ab7ceeb9facf8ae905d4adedc43b4a5\n",
      "Score: 0.71875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.15000000000000002\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.0\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.25\n",
      "num_filters_4: 128\n",
      "dropout_4: 0.1\n",
      "units: 192\n",
      "dense_activation: tanh\n",
      "dropout_5: 0.0\n",
      "learning_rate: 0.00023899521618106246\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.706250011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.30000000000000004\n",
      "num_filters_2: 64\n",
      "dropout_2: 0.0\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.35000000000000003\n",
      "num_filters_4: 256\n",
      "dropout_4: 0.25\n",
      "units: 128\n",
      "dense_activation: sigmoid\n",
      "dropout_5: 0.45\n",
      "learning_rate: 0.0019231287330200627\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.699999988079071\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.35000000000000003\n",
      "num_filters_2: 64\n",
      "dropout_2: 0.05\n",
      "num_filters_3: 64\n",
      "dropout_3: 0.25\n",
      "num_filters_4: 128\n",
      "dropout_4: 0.05\n",
      "units: 320\n",
      "dense_activation: sigmoid\n",
      "dropout_5: 0.25\n",
      "learning_rate: 0.0004543609940965399\n",
      "tuner/epochs: 14\n",
      "tuner/initial_epoch: 5\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: a8c1ad0634dce4c9a069bbcb81fae591\n",
      "Score: 0.6875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_filters_1: 16\n",
      "dropout_1: 0.0\n",
      "num_filters_2: 32\n",
      "dropout_2: 0.45\n",
      "num_filters_3: 128\n",
      "dropout_3: 0.05\n",
      "num_filters_4: 128\n",
      "dropout_4: 0.25\n",
      "units: 320\n",
      "dense_activation: tanh\n",
      "dropout_5: 0.45\n",
      "learning_rate: 0.0006105156037747876\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.6875\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 0.5709 - accuracy: 0.7300\n"
     ]
    }
   ],
   "source": [
    "# Show a summary of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model.\n",
    "loss, accuracy = best_model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bon là il y a encore plein de trucs\n",
    "- à améliorer (changer les différents paramètres jusqu'à trouver les meilleurs)\n",
    "- à ajouter : des représentations de la loss et de l'accuracy au cours des epochs, un pré-traitement des images, une liste des images avec pour chacun la probabilité de malignité, une fonction où on rentrer une unique image et il dit si elle est maligne ou bénine... \n",
    "Je pense qu'on peut vraiment faire des trucs funs !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Modèle pré-entraîné\n",
    "\n",
    "Enfin, on essaie d'améliorer la fiabilité de notre prédiction en utilisant un modèle pré-entraîné, à travers le principe du **transfer learning**. Le principe est d'utiliser les connaissances acquises par un réseau de neurones entraîné sur un grand nombre d'images, et d'appliquer ces connaissances à notre problème particulier (la reconnaissance de mélanomes). \n",
    "\n",
    "Pour ce faire, nous utiliserons **VGG16** : il s'agit d'un réseau de neurones construit par K. Simonyan et A. Zisserman (univeristé d'Oxford), considéré comme l'un des meilleurs dans le champs de la vision. Comme son nom l'indique, ce CNN est constitué de 16 couches. \n",
    "\n",
    "De plus, nous utiliserons des poids pré-établis pour ce CNN sur le dataset **ImageNet** (un jeu de données de plus de 14 millions d'images appartenant à 1 000 classes différentes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des modules \n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 prend en entrée des images de taille 224x224, il nous faut donc re-charger nos images (précédemment en taille 150x150) à la bonne taille :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size_VGG = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_jpg_train = Path_Projet_Melanomes+'/JPG_Sample_Train_Resize' \n",
    "path_jpg_test = Path_Projet_Melanomes+'/JPG_Sample_Test_Resize' \n",
    "    \n",
    "meta_data = df_sample\n",
    "meta_data = meta_data.set_index(\"image_id\")\n",
    "datasets = [path_jpg_train, path_jpg_test]\n",
    "\n",
    "output_VGG = []\n",
    "\n",
    "#itération sur chaque dataset (train puis test)\n",
    "for dataset in datasets :\n",
    "\n",
    "    files = os.listdir(dataset)\n",
    "    images = np.zeros((len(files), image_size_VGG, image_size_VGG, 3), dtype=np.float32)\n",
    "    labels = np.zeros(len(files))\n",
    "\n",
    "    # itération sur chaque image du dataset\n",
    "    for i, file in tqdm(enumerate(files)):\n",
    "        \n",
    "        # obtention du le chemin de l'image\n",
    "        img_path = os.path.join(dataset, file)\n",
    "\n",
    "        # obtention du label de l'image\n",
    "        label = meta_data.loc[file[0:12]].target\n",
    "\n",
    "        # ouverture de l'image et retraitement de l'image (changement de couleurs et réduction de taille)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (image_size_VGG, image_size_VGG))\n",
    "\n",
    "        # ajout de l'image et de son label à la liste de résultats\n",
    "        images[i, :] = (np.array(image) / 255).astype(np.float32)\n",
    "        labels[i] = label\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    output_VGG.append((images, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images_VGG, train_labels_VGG), (test_images_VGG, test_labels_VGG) = output_VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_VGG, train_labels_VGG = sklearn.utils.shuffle(train_images_VGG, train_labels_VGG, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit ensuite le réseau de neurones, à partir de VGG16 et des poids ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consutruction du CNN\n",
    "\n",
    "# les couches de base du CNN sont celles du modèle pré-entraîné, avec les poids issus de ce pré-entraînement (imagenet)\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(image_size_VGG, image_size_VGG, 3))\n",
    "\n",
    "# on ajoute ensuite quelques couches correspondant plus particulièrement à notre problème \n",
    "add_model = Sequential()\n",
    "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "add_model.add(Dense(256, activation='relu'))\n",
    "add_model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# on assemble les deux \n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compile le modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_VGG = np.asarray(train_labels_VGG).astype('float32').reshape((-1,1))\n",
    "test_labels_VGG = np.asarray(test_labels_VGG).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images_VGG, train_labels_VGG, batch_size=32, epochs=20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10 #à augmenter quand on aura le temps (ça prend 3h à tourner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing des images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=30, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1, \n",
    "        horizontal_flip=True)\n",
    "train_datagen.fit(train_images_VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_VGG = np.asarray(train_labels_VGG).astype('float32').reshape((-1,1))\n",
    "test_labels_VGG = np.asarray(test_labels_VGG).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images_VGG, train_labels_VGG, batch_size=batch_size, steps_per_epoch=train_images_VGG.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(test_images_VGG, test_labels_VGG),\n",
    "    callbacks=[ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_as_df = pd.DataFrame(history.history)\n",
    "hist_as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images_VGG)     # Vector of probabilities\n",
    "pred_labels = np.argmax(predictions, axis = 1) # We take the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = sklearn.metrics.confusion_matrix(pred_labels, test_labels_VGG)\n",
    "ax = plt.axes()\n",
    "sns.heatmap(CM, annot=True, \n",
    "           annot_kws={\"size\": 10}, \n",
    "           xticklabels=class_names, \n",
    "           yticklabels=class_names, ax = ax)\n",
    "ax.set_title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
